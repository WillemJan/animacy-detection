\documentclass[a4paper,UKenglish]{oasics}

\usepackage{microtype, graphicx, booktabs, multirow, color, colortbl}

\usepackage{blindtext}
\bibliographystyle{plain}

\definecolor{Gray}{gray}{0.9}
\usepackage[utf8]{inputenc}

\title{Animacy Detection in Stories}
\author[1]{Folgert Karsdorp}
\author[2]{Marten van der Meulen}
\author[3]{Antal van den Bosch}
\affil[1]{Meertens Institute\\
  Amsterdam, The Netherlands\\
  \texttt{folgert.karsdorp@meertens.knaw.nl}}
\affil[1]{Meertens Institute\\
  Amsterdam, The Netherlands\\
  \texttt{marten.van.der.meulen@meertens.knaw.nl}}
\affil[2]{Radboud University\\
  Nijmegen, The Netherlands\\
  \texttt{a.vandenbosch@let.ru.nl}}
\authorrunning{F. Karsdorp and M. van der Meulen and A. van den Bosch}
\Copyright{Folgert Karsdorp and Marten van der Meulen and Antal van den Bosch}
\subjclass{machine learning}
\keywords{machine learning}

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\serieslogo{}%please provide filename (without suffix)
\volumeinfo%(easychair interface)
  {Billy Editor and Bill Editors}% editors
  {2}% number of editors: 1, 2, ....
  {Conference/workshop/symposium title on which this volume is based on}% event
  {1}% volume
  {1}% issue
  {1}% starting page number
\EventShortName{}
\DOI{10.4230/OASIcs.xxx.yyy.p}% to be completed by the volume editor
% Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\maketitle

\begin{abstract}
\blindtext[1]
\end{abstract}

\section{Introduction}

For almost all species in the world, the capacity to distinguish
animate objects from inanimate objects is essential to their
survival. Those objects could be prey, for example, or predators or
mates. The fundamental nature of the distinction between animate and
inanimate appears in the early acquisition of the division. Children
of less than 6 months old are well able to make the division. Recent
brain research shows that the distinction appears in the organization
of the ventral temporal cortex in both humans and other primates. For
some researchers this provides evidence for the idea that the division
between animate and inanimate is an innate part of how we see the
world.

A common view supported by philosophers such as Daniel Dennett is that
animacy and aliveness are better to be treated as epistemological
stances rather than fixed states in the world. Not ineffable qualia
but behavioral capacity defines our stance towards objects. Depending
on whether people \textit{think} that an object is animate, they
utilize different cognitive strategies to explain and predict the
actions of those objects. The following opening of a well-known story
will make this point more clear:

\begin{quotation} {\it A farmer bought a pancake on the market. Once
    he got home, the farmer was hungry and began to bake the
    pancake. The farmer tried one of his skillful flipping techniques,
    but he failed and the pancake fell on the ground. Coincidentally,
    the door of the kitchen was open and the pancake rolled out to the
    field, as hard as he could\ldots}
\end{quotation}

As readers (or listeners) of a story, we choose to view participating
objects at varying levels of abstraction in order to predict their
behavior. Dennett \cite{dennett:96} defines three levels of
abstraction: (1) the \textit{physical stance}, (2) the \textit{design
  stance} and (3) the \textit{intentional stance}. The physical stance
deals with predictions about objects given their physical
properties. The design stance is slightly more abstract and deals with
concepts like purpose, function or design. Finally the intentional
stance is concerned with belief, thinking and intentions. These are
all cognitive strategies we use to predict and explain the actions of
objects in our environment. Interestingly, in the process of reading
the opening of the story about the fleeing pancake, readers and
listeners experience the transition from one strategy to the next
quite clearly. Initially, the pancake is interpreted from a physical
stance or perhaps the more abstract design stance in terms of the
purpose (i.e. to stave off hunger). It is only at the last adverbial
phrase `as hard as he could' that we start to wonder whether we should
adopt to the yet more abstract intentional stance and consider the
pancake to be a rational agent.

Given the fundamental nature of the distinction between animate and
inanimate, it is perhaps not too surprising that it has proven to be
useful in a variety of natural language processing tasks dealing with
syntax, semantics, parsing, information retrieval, etc. Existing
methods to automatically label natural language for animacy are
usually rule based or apply machine learning techniques or make a
combination of both. Common to most approaches is that they make use
of semantic lexicons with information about animacy and syntactic cues
in a text. Both feature types are relatively costly to obtain as they
require large vocabularies or syntactic parsing systems, which are
besides for a handful of western languages, often not available.

In this paper we present a new method to automatically label texts for
animacy. We will show that we can mostly do away with features that
require syntactic parsing or semantic lexicons while still yielding
competitive performance. We will focus on labeling animacy in stories
because stories pose some interesting problems to automatic systems of
animacy recognition. As the example of the fleeing pancake already
made clear, quite some entities in stories, though inanimate in the
`real' world, exhibit all behavioral capacities to consider them to be
animate and rational agents. Such examples make a clear case for
developing dynamic, data driven systems that do not rely too much on
static and fixed world knowledge.

The rest of this paper is structured as follows. We will start with an
short overview of existing techniques to automatically label texts for
animacy (\S\ref{sec:previous-work}). After a description a the corpus
used in our study and the annotations of the corpus have been
established (\S\ref{sec:data}), we will give an account of the
computational models used in the study in Section \ref{sec:models}. We
report on the empirical results in Section \ref{sec:results}. The last
section offers our conclusions and possible directions for future
research.


\section{Previous Work}\label{sec:previous-work}


In most of the literature on automatic animacy detection, people make
a binary distinction between animate and inanimate. \cite{bowman:12}
for example, define objects to be animate if they are alive and have
the ability to move under their own will. \cite{orasan:07} define
animacy in the context of anaphora resolution: ``if its referent can
also be referred to using one of the pronouns he, she, him, her, his,
hers, himself, herself, or a combination of such pronouns
(e.g. his/her)'' Not only are some parts of these definitions somewhat
circular, we believe that they are both essentialistic and are not
particularly suited for the rich and wondrous entities that live in
the realm of stories. We choose to define animacy in terms of
Dennett's intentional stance which comes down to the question whether
``you decide to treat the object whose behavior is to be predicted as
a rational agent'' \cite[17]{dennett:96}.

A handful of papers deal with automatic animacy detection. Most
approaches make use of rule-based systems or machine learning systems
with morphological and syntactic features. \cite{evans:00} present a
rule-based system that makes use of the lexical-semantic database
WordNet. They label each synset in WordNet for animacy. Using a
variety of rules to detect the head of an NP, the use the fraction of
synsets in which a particular noun occurs to arrive at a
classification for animacy. \cite{orasan:01} extend their previous
algorithm by first determining the animacy of senses from WordNet on
the basis of an annotated corpus. They then apply a $k$-nearest
neighbor classifier using a number of lexical and syntactic features
and features derived from WordNet to arrive at a final animacy
classification.

\cite{ovrelid:04,ovrelid:05,ovrelid:06,ovrelid:08,ovrelid:09} present
a number of animacy classifiers that make use of syntactic and
morphological features. Some of the features used are: how often has a
noun been analyzed as `subject' or `object'? How often does a noun
occur in a passive by-phrase? How often does a noun occur as a
subject, immediately followed by animate personal pronouns or
inanimate personal pronouns?  These features are then aggregated for
each lemma after which a machine learning system (decision tree or
$k$-nearest neighbor classifier) is trained. In \cite{bowman:12}
similar approach is presented. In this study a maximum entropy
classifier is trained on the basis of three feature types: (1)
bag-of-words with and without their corresponding Part-of-Speech tags,
(2) internal syntactic features such as the syntactic head and (3)
external syntactic features that describe the dependency relation of a
noun to a verb (i.e.\ subject relation, object relation etc.)  This is
the only study that makes use of a fully labeled corpus for animacy.
Not so much concerned with animacy detection, \cite{karsdorp:12}
attempt to extract the cast (i.e.\ all characters) from a
story. Similar to \cite{bowman:12} they rely on dependency tags to
extract the subjects of direct and indirect speech.

\cite{bloem:13} present a model that attempts to generalize the
animacy information in a lexical-semantic database of Dutch by
augmenting `non-ambiguous' animate entries with contextual information
from a large treebank of Dutch. They apply a $k$-nearest neighbor
algorithm with distributional lexical features that aim to capture the
association using measures like mutual information between a verb or
adjective and a particular noun. The idea is that nouns that occur in
similar contexts as animate nouns are more likely to be animate than
nouns that occur more frequently in contexts similar to inanimate nouns.

\cite{moore:13} present an interesting approach that combines a number
of animacy classifiers in a voting scheme and aims at an interpretable
and correctable model of animacy classification. They combine a
variety of classifiers such as the WordNet-based approach of
\cite{evans:00}, dictionary sources and systems for named entity
recognition.

% commentary on above approaches
There are a number of problems with the approaches mentioned
above. First, almost all of them heavily rely on costly features derived from
lexical-semantic databases or syntactic parsing. For most languages in
the world, however, we cannot rely on these resources. Second, animacy
detection is often seen as a useful feature for a range of natural
language processing techniques, such as anaphora resolution and
syntactic parsing. The mutual dependence of these techniques and
animacy detection, however, leads to a chicken and egg
situation. Another problem with the approaches above is that they are
lemma based which means that the models are generally insensitive to
different usages of a particular word. A pancake is typically not an
animate entity, but, as we have seen, whether it is animate or not is
only to be determined in relation to the story in which it appears. We
therefore need to develop a dynamic, data driven system that is token
based and does not rely too much on static and fixed world knowledge.

\section{The Data, Annotation and Preprocessing}\label{sec:data}

Our corpus consists of 74 Dutch stories from the collection
\textit{Volkssprookjes uit Nederland en Vlaanderen} compiled by
\cite{sinninghe:78}. The collection contains retellings of popular
stories from the Netherlands and Flanders, such as \textit{The
  Bremen Town Musicians} (ATU 130) and \textit{The Table, the Ass, and
  the Stick } (ATU 563). Some of the less well-know stories are
\textit{Cock, Hen, Duck, Pin, and Needle on a Journey} (AT 0210) and
(ATU 780) \textit{The Singing Bone}. A digital version of the
collection available in the Dutch Folktale Database from the Meertens
Institute (corpus SINVSUNV.20E).\footnote{See
  \url{http://www.verhalenbank.nl}} We chose this collection because
of its homogeneity while it still displays enough diversity: (1) all
tales are written in standard Dutch, (2) the stories are edited by the
same editor, (3) the tales have a comparable length and (4) the
collection contains tales from a number of different genres, such as
fairy tales, legends, riddles etc.

We assigned to each word in the collection a binary label `animate' or
`inanimate'. 11542 words were labeled as animate and 62926 words as
inanimate. The vocabulary size is 5549 with 743 unique animate words
and 5011 unique inanimate words. The annotations consist of 4627
animate words that have been tagged as noun or name against 6878
inanimate words.

After tokenization with the tokenization module of the Python software
package Pattern \cite{smedt:12} we fed all stories to the
state of the art syntactic parser for Dutch, Alpino \cite{bouma:01}.

\section{Experimental Setup}\label{sec:models}
This section describes our experimental setup including the features
used, the machine learning models we applied and our methods of
evaluation.\footnote{The data set and the code to perform the
  experiments are made available online at
  \url{https://fbkarsdorp.github.io/animacy-detection}}.

\subsection{Task description}

\subsection{Features}

We explore a range of different features and feature combinations
including lexical features, morphological features, syntactic features
and semantic features.

\subparagraph*{Lexical features}
We take a sliding window approach where for each focus word (i.e.\ the
word for which we want to predict whether it is animate or not) we
extract $n$ words to the left, $n$ words to the right and the focus
word itself. In all experiments we set $n$ to 3. In addition to the
word forms, we extract for each word in a window the lemmata
corresponding to the words.

\subparagraph*{Morphological Features}
For each word we extract its Part-of-Speech tag. For reasons of
comparability for choose to use the tags as provided by the
output of the syntactic parser, instead of a more specialized
Part-of-Speech tagger. Again, we take a sliding window approach and
extract the Part-of-Speech tags for $n$ word left of the focus word
and $n$ word right of the focus word as well as the tag of the focus
word itself.

\subparagraph*{Syntactic Features}
We extract the dependency tag for each word and its $n$ neighbors to
the right and to the left as provided by the syntactic parser
Alpino. Animate entities tend to take subject or object positions in a
sentences which is why this feature is expected and has proven to
perform rather well.

\subparagraph*{Semantic Features}
For each focus word we add a 300 dimension vector representation that
was learned by applying word2vec on the Dutch version of CoW. Missing
words were given a vector were all dimensions are set to zero.


\subsection{Models}
We make use of a Logistic Regression classifier also know as a Maximum
Entropy classifier with L2 regularization as implemented in
\cite{sklearn}. In all experiments, we set the regularization strength
parameter $C$ to 1.

We compare nine models in which we make use of different feature
combinations: (1) words, (2) words and Part-of-Speech tags, (3)
words, Part-of-Speech tags and lemmata, (4) words, Part-of-Speech
tags, lemmata and dependency tags, (5) word embeddings and (6-9) the
features in model 1 to 4 plus word embeddings.

\subsection{Evaluation}

Animate words are far outnumbered by the inanimate words in our
collection. Reporting accuracy scores would therefore provide
distorted results to much in favor of the majority category. The
rarity of animate words make evaluation measures such as the
well-known F1-score more appropriate. For all experiments we report on
the precision, recall and F1-score \cite{rijsbergen:79}. In most of
the literature on animacy detection results are only given for the
classification of nouns. We will report the results for all words in a
text and on those specifically for nouns separately.

Story-based training!

\section{Results}\label{sec:results}



\begin{table*}
\centering
\begin{tabular}{llrrrr}
\toprule
features                                                 &           & precision & recall & $F1$ \\ \midrule
\multirow{2}{*}{word}                                    & animate   & 0.93      & 0.80   & 0.86 \\
                                                         & inanimate & 0.96      & 0.99   & 0.98 \\
\multirow{2}{*}{word + PoS}                              & animate   & 0.92      & 0.88   & 0.90 \\
                                                         & inanimate & 0.98      & 0.99   & 0.98 \\
\multirow{2}{*}{word + PoS + lemma}                      & animate   & 0.92      & 0.88   & 0.90 \\
                                                         & inanimate & 0.98      & 0.99   & 0.98 \\
\multirow{2}{*}{word + PoS + lemma + dep}                & animate   & 0.93      & 0.88   & 0.90 \\
                                                         & inanimate & 0.98      & 0.99   & 0.98 \\
\multirow{2}{*}{word + embeddings}                       & animate   & 0.93      & 0.89   & 0.91 \\
                                                         & inanimate & 0.98      & 0.99   & 0.98 \\
\rowcolor{Gray}                                          & animate   & 0.93      & 0.93   & 0.93 \\
\rowcolor{Gray}\multirow{-2}{*}{word + PoS + embeddings} & inanimate & 0.99      & 0.99   & 0.99 \\
\multirow{2}{*}{word + PoS + lemma + embeddings}         & animate   & 0.93      & 0.93   & 0.93 \\
                                                         & inanimate & 0.99      & 0.99   & 0.99 \\
\multirow{2}{*}{word + PoS + lemma + dep + embeddings}   & animate   & 0.92      & 0.93   & 0.93 \\
                                                         & inanimate & 0.99      & 0.99   & 0.99 \\
\multirow{2}{*}{embeddings}                              & animate   & 0.90      & 0.76   & 0.83 \\
                                                         & inanimate & 0.96      & 0.98   & 0.97 \\

\bottomrule
\end{tabular}
\caption{Precision, recall and $F1$ scores for animate and inanimate classes per feature settings for all words.}
\label{tab:results-all}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{llrrrr}
\toprule
features                                               &           & precision & recall & $F1$ \\ \midrule
\multirow{2}{*}{word}                                  & animate   & 0.64      & 0.96   & 0.77 \\
                                                       & inanimate & 0.98      & 0.81   & 0.89 \\
\multirow{2}{*}{word + PoS}                            & animate   & 0.80      & 0.92   & 0.86 \\
                                                       & inanimate & 0.95      & 0.89   & 0.92 \\
\multirow{2}{*}{word + PoS + lemma}                    & animate   & 0.80      & 0.92   & 0.86 \\
                                                       & inanimate & 0.95      & 0.89   & 0.92 \\
\multirow{2}{*}{word + PoS + lemma + dep}              & animate   & 0.80      & 0.93   & 0.86 \\
                                                       & inanimate & 0.96      & 0.88   & 0.92 \\
\multirow{2}{*}{word + embeddings}                     & animate   & 0.83      & 0.93   & 0.88 \\
                                                       & inanimate & 0.96      & 0.90   & 0.93 \\
\rowcolor{Gray}                                        & animate   & 0.92      & 0.92   & 0.92 \\
\rowcolor{Gray}\multirow{-2}{*}{word + PoS + embeddings} & inanimate & 0.95    & 0.95   & 0.95 \\
\multirow{2}{*}{word + PoS + lemma + embeddings}       & animate   & 0.92      & 0.92   & 0.92 \\
                                                       & inanimate & 0.95      & 0.95   & 0.95 \\
\multirow{2}{*}{word + PoS + lemma + dep + embeddings} & animate   & 0.91      & 0.92   & 0.92 \\
                                                       & inanimate & 0.95      & 0.95   & 0.95 \\
\multirow{2}{*}{embeddings}                            & animate   & 0.66      & 0.88   & 0.75 \\
                                                       & inanimate & 0.94      & 0.81   & 0.87 \\

\bottomrule
\end{tabular}
\caption{Precision, recall and $F1$ scores for animate and inanimate classes
  per feature settings for all words tagged as noun.}
\label{tab:results-noun}
\end{table*}

\section{Concluding Remarks}

\subparagraph*{Acknowledgments}

The work on which this paper is based has been supported by the
Computational Humanities Programme of the Royal Netherlands Academy of
Arts and Sciences, under the auspices of the Tunes \& Tales
project. For further information, see \url{http://ehumanities.nl}.

\bibliography{paper}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
